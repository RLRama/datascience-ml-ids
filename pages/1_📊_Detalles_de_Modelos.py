import streamlit as st

st.set_page_config(
    page_title="Informaci√≥n de modelos",
    page_icon="ü§ñ",
)

st.image(
    "https://emojipedia-us.s3.dualstack.us-west-1.amazonaws.com/thumbs/120/apple/325/open-book_1f4d6.png",
    width=100
)

def wikia():
    st.title("Modelos de aprendizaje utilizados")
    st.header("M√°quina de vector soporte")
    st.markdown(
        """
        Las m√°quinas de vectores se soporte o m√°quinas de vector soporte son un conjunto de aprendizaje supervisado desarrollados por Vladimir Vapnik y su equipo en los laboratorios de AT&T Bell.
        Est√°n relacionados con problemas de clasificaci√≥n y regresi√≥n.
        Se le da un conjunto de ejemplos de entrenamiento (muestras) y podemos etiquetar las clases y entrenar un SVM para construir un modelo que prediga la clase de una nueva.
        Por lo tanto, dado un conjunto de puntos, subconjunto de un conjunto mayor (espacio), en el que cada uno de ellos pertenece a una de dos posibles categor√≠as, y el propio algoritmo construye un modelo capaz de predecir si un punto nuevo (categor√≠a que desconocemos) pertenece a una categor√≠a o a la otra.
        Este tipo de algoritmo pertenece a la familia de clasificadores lineales.
        Est√° estrechamente relacionado con las redes neuronales.
        - Al usar una funci√≥n kernel, resultan un m√©todo de entrenamiento alternativo para clasificadores polinomiales, funciones de base radial y perceptr√≥n multicapa.
        """
    )
    st.header("Regresi√≥n log√≠stica")
    st.markdown(
        """
        En estad√≠stica, la regresi√≥n es un tipo de an√°lisis de regresi√≥n utilizado para predecir utilizado para predecir el resultado de una variable categ√≥rica (una variable que puede adoptar un n√∫mero limitado de categor√≠as) en funci√≥n de la variables independientes o predictoras.
        Es √∫til para modelar la probabilidad de un evento ocurriendo en funci√≥n de otros factores. El an√°lisis de regresi√≥n log√≠stica se enmarca en el conjunto de Modelos Lineales  Generalizados que usa como funci√≥n de enlace la funci√≥n ‚Äúlogit‚Äù.
        Las probabilidades que describen la posici√≥n resultado de un √∫nico ensayo se modelan como una funci√≥n de variables explicativas, utilizando una funci√≥n log√≠stica.
        La regresi√≥n log√≠stica es usada extensamente en las ciencias m√©dicas y sociales. Otros nombres para regresi√≥n log√≠stica usados en varias √°reas de aplicaci√≥n incluyen modelo log√≠stico, modelo logit, y clasificador de m√°xima entrop√≠a.
        """
        )
    st.header("Bosque aleatorio")
    st.markdown(
        """
        Es una combinaci√≥n de √°rboles predictores tal que cada √°rbol depende de los valores de un vector aleatorio probado independientemente y con la misma distribuci√≥n para cada uno de estos.
        Es una modificaci√≥n sustancial de bagging que construye una larga colecci√≥n de √°rboles no correlacionados y luego los promedia.
        La idea esencial del bagging es promediar muchos modelos ruidosos aproximadamente imparciales, y por tanto reducir la variaci√≥n. 
        Los √°rboles son los candidatos ideales para el bagging, dado que ellos pueden registrar estructuras de interacci√≥n compleja en los datos, y si crecen suficientemente profundo, tienen relativamente baja parcialidad. Producto de que los √°rboles son notoriamente ruidosos, ellos se benefician enormemente al promediar.
        Siguen el siguiente algoritmo:
        1. Sea N el n√∫mero de casos de prueba, M es el n√∫mero de variables en el clasificador.
        2. Sea m el n√∫mero de variables de entrada a ser usado para determinar la decisi√≥n en un nodo dado; m debe ser mucho menor que M.
        3. Elegir un conjunto de entrenamiento para este √°rbol y usar el resto de los casos de prueba para estimar el error.
        4. Para cada nodo del √°rbol, elegir aleatoriamente variables en las cuales basar la decisi√≥n. Calcular la mejor partici√≥n del conjunto de entrenamiento a partir de las variables.
        """
    )

    st.header("k-Nearest Neighbor")
    st.markdown(
        """
        Es un algoritmo basado en instancia de tipo supervisado de ML. Puede usarse para clasificar nuevas muestras(valores discretos) o para predecir(regresi√≥n,valores continuos).
        Sirve para clasificar valores buscando los puntos de datos ‚Äúsimilares‚Äù aprendidos en la etapa de entrenamiento y haciendo conjeturas de nuevos puntos basado en esa clasificaci√≥n.
        Busca observaciones m√°s cercanas a la que se est√° tratando de predecir y clasificar el punto de inter√©s basado en la mayor√≠a de datos que le rodean.
        Se aplica en Sistemas de recomendaci√≥n, b√∫squeda sem√°ntica y detecci√≥n de anomal√≠as.
        #### Ventajas
        Es sencillo de aprender e implementar
        #### Desventajas
        Utiliza todo el dataset para entrenar ‚Äúcada punto‚Äù y por eso requiere de uso de mucha memoria y recursos de procesamiento.
        #### ¬øC√≥mo funciona?
        1. Calcular la distancia entre el √≠tem a clasificar y el resto de √≠tems del dataset de entrenamiento,
        2. Seleccionar los ‚Äúk‚Äù elementos m√°s cercanos.
        3. Realizar una ‚Äúvotaci√≥n de mayor√≠a‚Äù entre los k puntos: los de una clase/etiqueta que ‚Äúdominen‚Äù decidir√°n su clasificaci√≥n final.
        Teniendo en cuenta esto, para decidir la clase de un punto es importante el valor de K, ya que determinar√° a qu√© grupo pertenecen los puntos, sobre todo las fronteras entre grupos.
        Para medir la ‚Äúcercan√≠a‚Äù entre puntos est√°n la [distancia Euclidiana](https://es.wikipedia.org/wiki/Distancia_euclidiana) o la [Cosine Similarity](https://es.wikipedia.org/wiki/Similitud_coseno).

        """
    )

    st.header("Clasificador bayesiano ingenuo")
    st.markdown(
        """
        Es uno de los algoritmos m√°s simples y poderosos para la clasificaci√≥n basado en el Teorema de Bayes con una suposici√≥n de independencia entre los predictores. Es f√°cil de construir y √∫til para conjuntos de datos muy grandes.
        El clasificador asume que el efecto de una caracter√≠stica particular en una clase es independiente de otras caracter√≠sticas.
        La f√≥rmula es:
        """)
    st.image("https://live.staticflickr.com/65535/47792141631_5788f52f0c_b.jpg")
    st.markdown(   
        """
        - P(h): es la probabilidad de que la hip√≥tesis h sea cierta (independientemente de los datos). Esto se conoce como la probabilidad previa de h.
        - P(D): probabilidad de los datos (independientemente de la hip√≥tesis). Esto se conoce como probabilidad previa.
        - P(h|D): es la probabilidad de la hip√≥tesis h dada los datos D. Esto se conoce como la probabilidad posterior.
        #### Pasos para que el clasificador Naive Bayes calcule la probabilidad de un evento:
        1. calcular la probabilidad previa para las etiquetas de clase dadas.
        2. determinar la probabilidad de probabilidad con cada atributo para cada clase.
        3. poner estos valores en el teorema de Bayes y calcular la probabilidad posterior.
        4. ver qu√© clase tiene una probabilidad m√°s alta, dado que la variable de entrada pertenece a la clase de probabilidad m√°s alta.
        #### Ventajas
        - Es f√°cil y r√°pido predecir la clase de conjunto de datos de prueba. Tambi√©n funciona bien en la predicci√≥n multiclase.
        - Cuando se mantiene la suposici√≥n de independencia, un clasificador Naive Bayes funciona mejor en comparaci√≥n con otros modelos como la Regresi√≥n Log√≠stica y se necesitan menos datos de entrenamiento.
        - Funciona bien en el caso de variables de entrada categ√≥ricas comparada con variables num√©ricas.
        #### Desventajas
        - Si la variable categ√≥rica tiene una categor√≠a en el conjunto de datos de prueba, que no se observ√≥ en el conjunto de datos de entrenamiento, el modelo asignar√° una probabilidad de 0 y no podr√° hacer una predicci√≥n. Esto se conoce a menudo como frecuencia cero. Para resolver esto, podemos utilizar la t√©cnica de alisamiento.
        - Otra limitaci√≥n de Naive Bayes es la asunci√≥n de predictores independientes. En la vida real, es casi imposible que obtengamos un conjunto de predictores que sean completamente independientes.
        """
    )
    

wikia()